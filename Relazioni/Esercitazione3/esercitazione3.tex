\raggedright
\mychapter{3}{Work Project 3}
	\label{ch:cc}
	\section{Esercizio 1: Logica Proposizionale}
		\label{sec:es1}
		La logica proposizionale è un linguaggio formale utilizzato dagli agenti logici per la rappresentazione della conoscenza. Un agente intelligente che sfrutta tale approccio si ispira al processo umano di deduzione a partire da avvenimenti noti che ha precedentemente appresso. Quindi l'agente avrà bisogno di una base di conoscenza (\texttt{KB: knowledge base}), in cui archiviare le informazioni utilizzate nella deduzione, e di un meccanismo di inferenza (\texttt{inference engine}) per effettuare le deduzioni.
		\par
		L'agente, basato sulla conoscenza, ogni volta che innesca il processo deduttivo, enuncia alla base di conoscenza la percezione corrente, dopodiché chiede ad essa l'azione da eseguire, la scelta avverrà in base alla informazioni archiviate. Infine comunica  di aver eseguito l'operazione, in modo tale da far in modo che l'evento possa ampliare la propria base di conoscenza. Inizialmente l'agente può non essere a conoscenza di tutto ciò di cui necessita per completare la sua operazione, quindi sarà necessario una fase di raccolta delle informazioni iniziali. Possiamo fornire questa conoscenza al nostro agente in due modi, mediante un approccio dichiarativo, vi è proprio una fase di raccolta di informazioni da parte dell'agente, oppure nel caso di un approccio procedurale possiamo fornire noi una conoscenza iniziale all'agente. Solitamente i due approcci vengono usati insieme in modo tale da avere una buona base di conoscenza.
		\par
		Le informazioni non possono essere salvate nella base di conoscenza dell'agente in un linguaggio naturale, o comunque in un qualsiasi modo da cui non sia possibile trarre delle conclusioni. Per tale motivo le frasi, o formule, che costituiscono il KB, sono espresse in un linguaggio formale di rappresentazione della conoscenza. Esso prevede una sintassi, per esprimerle in maniera corretta, ed una semantica, che gli attribuisce un significato, possiamo vedere la semantica come un legame tra il mondo reale e quello della rappresentazione.
		%che ne definisce la verità rispetto ad ogni mondo possibile%.
		La stessa identica frase espressa naturalmente può essere tradotto in modi differenti in base alla diversa sintassi della logica che andiamo ad utilizzare. Però il risultato da ottenere sarà lo stesso poichè è indipendente dalla logica utilizzata per la traduzione.
		% cioè avendo dei dati noti a priori, capire se si può riuscire a dedurre altre informazioni che a noi interessa sapere, tale relazione è detta di implicazione(entailment).
		Utilizzeremo la logica proposizionale, in cui la sintassi è composta da frasi atomiche, legate tra loro tramite connettivi logici per ottenere frasi più complesse, il singolo elemento della frase può assumere valore vero o falso,infatti la semantica di tale logica è quella di trovare il valore vero di ogni frase dato la conoscenza nota a priori. Le operazioni tra i letterali sono: la \textbf{negazione} che muta il valore della nostro letterale; l' operazione di \textbf{and} che ci da vero solo se i due connettivi sono veri;l'operazione di \textbf{or} restituisce vero se uno dei due è vero;l' \textbf{implicazione} ci restituisce falso se dal vero deduciamo il falso e infine quella \textbf{bicondizionale} vera se e solo se entrambi i letterali hanno lo stesso significato.
		Per verificare la relazione di implicazione possiamo ricorrere a due strade, la prima è quella in cui enunciamo tutti i possibili mondi del nostro modello, per esempio tramite la tabella di verità, tra questi individuiamo quelli che rispettano la nostra base di conoscenza e la deduzione che stiamo facendo e se i mondi che rispecchiano la nostra percezione sono contenuti all' interno del concetto che vogliamo dedurre, allora possiamo dire che la nostra conoscenza implica quel fatto.
		Sulla tabella possiamo vedere facilmente le implicazioni osservando che quando sono vere le condizioni che portano a quella deduzioni allora anche ciò che vogliamo dedurre è vero.
		\par
		Questo metodo è molto oneroso quando i possibili mondi sono tanti, enumerarli tutti sarebbe molto dispendioso sia in termini di spazio che di tempo, per tale ragione di preferiscono altre strade.
		\par
		Una possibile è scrivere le frasi in forma CNF e da li applicare metodi inferenziali per capire se la nostra conoscenza implica il fatto. 
		Prima di tutto però dobbiamo trasformare le frasi che abbiamo nella forma CNF, cioè or di congiunzioni atomiche, per farlo sfruttiamo le regole di equivalenza logica. Purtroppo anche con questo metodo c'è un grande sforzo dell' essere umano che deve applicare tali regole affinché sia possibile dedurre qualcosa.
		Quando la conoscenza, opportunamente rappresentata, è nella forma da noi voluta, possiamo provare a dimostrare che la nostra conoscenza, implicata dal negato della frase che vogliamo da cui effettuiamo la deduzione non sia soddisfacibile, cioè non esista nessun modello che implica quella frase. Se ciò accade significa che la frase non negata è valida, cioè in tutti i mondi in cui è vera la nostra base di conoscenza è vera anche la frase. Per verificare che not A sia insoddisfacibile, basta prendere una coppia di clausole(congiunzione di letterali), presenti nella nostra base di conoscenza ed iniziare a derivare nuove informazioni, ciò avviene quando nella frase che si viene a creare si presentino due letterali complementari (A e not A).
		Applicando questo ragionamento ricorsivamente fino a quando non riusciamo più a determinare nuove clausole e quindi quel fatto non è deducibile dalla nostra conoscenza, oppure se si arriva alla clausola nulla possiamo affermare che la not A non è soddisfacibile.
		Tali situazioni implicano la validità della clausola di partenza A.
		\par
		Tutti e due algoritmi sono completi e soundness.
		Un algoritmo è completo se é capace di enumerare tutte le relazione di implicazione possibili dalla base di conoscenza.
		Un algoritmo è soundness quando ogni implicazione dell' algoritmo, sono vere anche nel caso della base di conoscenza.
		Per entrambe le procedure enunciate vale il discorso che l' agente non conosce il significato della frase, quello è dato dall' essere umano, per l' agente una frase vale un' altra, esso è solo capace di applicare le regole inferenziali.
		
		\section{Esercizio 2: Reti Bayesiane}
		\label{sec:es2}
		L' approccio probabilistico, rispetto alle altre tipologie tiene conto di un grado di incertezza.Infatti a differenza di un approccio logico, in cui un fatto può essere vero o falso, nell'approccio probabilistico si considera la probabilità di quell' avvenimento. Tale probabilità non ci dice a priori se sia vero oppure no, per esempio affermando che oggi pioverà con una probabilità di 0.99 potrebbe capitare che non piova, proprio perché la probabilità definisce il livello di incertezza rispetto ad un conoscenza da noi acquisita che varia da persona a persona (agente ad agente), dipendete dall'ipotesi che si possono avere esperienze diverse che portano ad arrivare a valori di incertezza diversi.Le reti Bayesiane si rifanno al concetto di probabilità, più propriamente alla regola di Bayes che si enuncia nel sugente modo: $P(causa|effetto)=\frac{P(effetto|causa)*P(causa)}{p(effetto)}$. Una rete Bayesiana di solito è fatta in questo modo:
		\includegraphics[width=0.95\textwidth, height=0.40\textheight]{retebayesiana.jpg}
		come osserviamo ci sono dei nodi, che determinano la variabile di cui vogliamo tenere conto a cui si assegnano anche i valori di probabilità a priori, e dei collegamenti che indicano la relazione di causalità tra i nodi, determinata dalla freccia che va da un nodo verso un altro.
		\par 
		Per realizzare tale rete dobbiamo tenere conto delle relazioni di causalità delle nostre variabili, così facendo otteniamo una rete che riesce ad esplicare bene le nostre conoscenze.Una peculiarità di questa rete è la compattezza, infatti qualsiasi altro modo di realizzare la rete ne aumenterebbe la complessità.Una rete così realizzata permette di effettuare calcolo delle probabilità in modo semplice, perché  in una rete siffatta vale l' indipendenza condizionata, una volta che definitp un valore per una variabile padre, le variabili figlie sono indipendenti tra di loro. Per esempio se la variabile A è padre di B e C la probabilità $P(B and C)=P(B|A)*P(C|A)$, se non avessimo l' indipendenza non potremmo scrivere i due domini separati, ciò implicherebbe che dovremmo tener conto anche della probabilità di B dato C. Per queste reti si ha anche il concetto di semantica locale, cioè ogni nodo è indipendente dai suoi non discendenti se viene dato il nodo padre; e della coperta di Markov, un nodo è indipendente da tutti se dato il nodo padre, i figli e i padri dei figli.
		\par  
		Per ricavare dati da queste reti date le probabilità a priori, possiamo semplicemente calcolare tutti i valori di probabilità a posteriori che ci occorrono, metodo complesso nel caso in cui la rete sia grande e il numero di possibili valori delle variabili è elevato. Di solito si può ricorrere all' approccio frequentistico, cioè generiamo un token, quest' ultimo avrà una probabilità di ottenere un valore del nodo pari proprio all' incertezza legata ad esso,dopodichè si inizia a campionare la rete facendo scorrere il token su tutta la rete, fissando il valore dei nodi della rete con quelli risultanti dal campionamento. La configurazione dei valori ricavati, una volta raggiunti la quantità di risultati voluti, si divide il numero di configurazioni ottenute per quello dei campioni valutati. Tale metodo per un alto numero di eventi determinati deve tendere all' approccio di calcolare le probabilità a posteriori. Le reti bayesiane così costruite sono statiche perché non tengono conto di un riferimento temporale, infatti esistono le reti bayesiane dinamiche che tengono conto anche di un valore temporale, suddiviso in istanti. Quindi la variabile casuale non ha solo relazioni con altre variabili, ma anche con se stessa però valutata in istanti diversi di tempo. I valori di probabilità di solito che vengono ricavate su tali reti sono: il \texttt{filtering} cioè la probabilità che avvenga un determinato evento all' istante t dati gli eventi precedenti e l' evidenza; lo \texttt{smoothing} la probabilità che un evento in un istante t-k con k che va da 0 a t data l' evidenza fino all' istante t, utilizzato per correggere la probabilità con cui si va da un fatto in un istante ad un altro;
		la \texttt{previsione}, la probabilità di un evento all' istante t+k con k>1 dati gli eventi e l' evidenza fino a t;
		la \texttt{spiegazione migliore} che data la probabilità all' istante t di eventi, evidenza quali sono quei determinati valori che ci permettono di avere quella probabilità.
		\par 
		Per esempio il \texttt{filtering\} può essere facilmente calcolato ricorsivamente, infatti conosciuto il fatto iniziale ed essendo i processi stazionari, 
		possiamo successivamente determinare quello all' istante successivo e poi pesare la probabilità dello stato data l' evidenza.
		Questo è possibile per la stazionarità cioè per passare da un fatto ad un altro la tabella della probabilità non cambia e quindi possiamo affermare che la legge che prevede tale cambiamento è sempre la stessa.
		Esiste anche una tecnica detta di \texttt{particle filtering} in cui faccio un campionamento sul fatto iniziale, propago tali esempi sul campione successivo li peso per l' evidenza e rifaccio il campionamento per lo stato successivo tenendo conto del nuovo peso dei campioni. Ovviamente per ottenere il valore di probabilità di una determinata configurazione, dobbiamo dividere il numero di casi uguali per il totale di campionamenti effettuati e per un alto valore di di quest' ultimi la mia probabilità tende a quella reale che avrei ottenuto con il filtering. Queste reti possono essere usate anche nel machine learning. Dato un insieme di ipotesi possiamo vedere qual' è la probabilità di una di queste data un evidenza, cioè vogliamo calcolare la probabilità di $P(hi|d)$ dove hi è l' ipotesi e d è l' evidenza, ovviamente ad ogni nuovo valore potrebbe variare la hi che massimizza la probabilità cambia. Ovviamente non è molto agevole portare avanti i calcoli per tutte le ipotesi, per tale ragione si sceglie quella che massimizza il valore a posteriori (\texttt{MAP}), cioè quella ipotesi che massimizza $\alpha*P(d|hi)*p(hi)$ dove $\alpha$ è il valore di normalizzazione. Si può anche massimizzare il logaritmo di tale funzione, nel caso in cui le ipotesi hanno probabilità a priori (stessa complessità) uguali si può usare la \texttt{maximum-likelihood (ML)}, in cui basta solo massimizzare $p(d|hi)$.
		L'uso dei logaritmi nel caso in cui compaiano esponenziali rende la massimizzazione della derivata molto più facile.
		
		\section{Esercizio 3: Reti Neurali}
			\label{sec: es3}
			Le reti neurali sono un meccanismo computazionale che si ispira al funzionamento del cervello umano. Sono rappresentabili tramite un grafo orientato, i cui nodi costituiscono un modello matematico del neurone, e agli archi orientati è associato un peso. Ogni nodo \textit{i} calcola la sua uscita $a_{i}$ applicando una funzione di attivazione \textit{g},anche chiamata \texttt{activation function}, alla somma pesata degli ingressi.% propagatisi dai nodi \textit{j} tramite l'arco corrispondente di peso $W_{i,j}$.
			\begin{equation}
			a_{i} = g(in_{i}) =g( \sum_{j=0}^n W_{j,i} a_{j})
			\end{equation}
			\medskip
			\begin{center}
				\includegraphics[width=0.8\textwidth, height=0.3\textheight]{neurone.jpg}
			\end{center}
			La funzione di attivazione storicamente utilizzata era quella di \emph{soglia}; in seguito si è diffusa la funzione \emph{sigmoide}, continua e derivabile rispetto alla precedente.\par
			Possiamo classificare strutturalmente le reti neurali distinguendo le \texttt{Recurrent Networks} e le \texttt{Feed-Forward Networks}. Le ultime non si fondano sul concetto di stato interno, a differenza delle prime,queste possono essere suddivise in più livelli, o \emph{layers}. Le reti neurali a singolo layer riescono a rappresentare solo operazioni \emph{linearmente separabili}, a due layer tutte le funzioni sono continue e quelle a tre sono discontinue.
			\begin{center}
				\includegraphics[scale=0.6]{RN-multilayer.jpg}
			\end{center}
			\par
			Una difficoltà, non trascurabile, risiede nella costruzione della struttura della rete neurale. Esse infatti sono sistemi \texttt{black-box}, il cui funzionamento interno è di difficile interpretazione logica, anche se è possibile averne un interpretazione matematica che non ci aiuta nella costruzione.
			\par
			Per determinare il peso degli archi si utilizza l'algoritmo di \texttt{Back-Propagation}. Si considera una coppia ingresso-uscita del data-set e si costituisce una rete con pesi casuali, o uniformi per abolire l' onere computazionale. L'errore tra l'uscita ottenuta e quella desiderata verrà propagato all'indietro,per ogni  livello, così da modificare i pesi ed ottenere una migliore approssimazione. Inoltre si fa uso del \texttt{learning rate} per far si che la propagazione di un singolo esempio non influisca interamente sui valori dei pesi, altrimenti questi tenderebbero ad oscillare di molto nel caso in cui ogni esempio ci fornisce un alto valore di errore. La procedura viene iterata per ogni esempio, ciascuno dei quali consentirà una correzione dei pesi, delineando una struttura della rete coerente con la funzione da approssimare. I problemi principali che riscontriamo in questo algoritmo sono l'eccessiva complessità temporale e la possibilità di arresto su di un minimo locale.\par
			Per quanto riguarda, invece,la determinazione del numero di neuroni, si procede tipicamente con un approccio \texttt{trial and error}, perché come già detto non è chiaro qual' è il significato della rete, per tale motivo si parte da reti semplici, se queste dopo la \texttt{Back-Propagation} funzionano bene, le si usa, altrimenti si iniziano a inserire nuovi neuroni o layer di questi e si continua a testare la validità delle nuove reti create.\par
			In definitiva, le reti neurali si sono rivelate empiricamente un buon approssimatore di funzioni, ed oggi il loro utilizzo ha ripreso vigore nella forma di \texttt{Deep Learning}.Tali tecniche ottengono un basso \emph{error rate} nei problemi relativi al riconoscimento di caratteri e sono largamente diffuse nei sistemi multimediali per quanto concerne il riconoscimento di immagini, audio o nella \texttt{Computer Vision} in generale. Seppur presentando buoni risultati, bisogna ricordare che \emph{non esiste un algoritmo di apprendimento migliore in assoluto, ma bisogna trovare quello più adatto al problema dato.}
			
		\section{Esercizio 4: Cloud e Crowdsourcing}
			\label{sec:es4}
			Il \texttt{Cloud Computing} ed il \texttt{Crowdsourcing} sono paradigmi affermatisi nel nuovo millennio, figli della dirompente rivoluzione della rete non solo al livello culturale, ma anche economico, fornendo nuovi modelli per sviluppare ed offrire servizi al cliente.\par
			Il cloud prevede l'erogazione di risorse informatiche \emph{on demand} attraverso Internet. Tipicamente, in linea del tutto generale, un \texttt{Cloud Provider} fornisce un \emph{pool} condiviso di risorse; esse possono essere configurate da un cliente amministratore che le carica di valore aggiunto; infine l'utilizzatore finale usufruisce delle risorse in tal modo configurate per poi rilasciarle al termine del suo utilizzo.\par
			Il crowdsourcing è lo sviluppo collettivo di un progetto da parte di volontari esterni al suo ideatore. Questo fenomeno deriva dalla nascita delle prime \emph{open community} di condivisione, profondamente legato al concetto di \emph{sharing} portato dalla rete  in tutti i campi. La \emph{community open source} è stata la prima a sfruttare i benefici del crowdsourcing.Questo paradigma si è diffuso anche in ambito aziendale con il modello di business di \emph{open enterprise}.\par
			Quantopian applica questi concetti fornendo i servizi per scrivere algoritmi di trading e rendendoli disponibili all'intera community, nella quale si potranno condividere idee, codice e dati. Esso, inoltre, investe un capitale sugli algoritmi con le migliori performance, condividendo i profitti con l'autore. Rappresenta, dunque, un modello di business all'avanguardia, che sfrutta pienamente le potenzialità della community, guidandola tramite i propri strumenti e ricavando introiti dall'impulso creativo a cui il crowdsourcing porta.
			\par 
			Tale piattaforma offre molti dataset direttamente senza importarli da altre fonti o di dover andare alla ricerca compulsiva sul web. Nel nostro esempio vediamo operazioni di trading sulle azioni americane, di una nostra possibile società che ha un certo numero di azioni iniziali e decide quando vendere o comprare tali titoli per aumentare i suoi introiti partendo da un capitale iniziale ed osservando cosa accade in un determinato lasso di tempo.
			\par 
			Il codice fornisce tale output:
			\includegraphics[width=1.0\textwidth, height=0.40\textheight]{inizio.png} 
			la linea rossa definisce il reale valore che si avrebbe avuto confrontandosi con il mercato, invece quello blu determina la previsione stimata dell' algoritmo , il valore position ci dice il numero di azioni attive, cioè non ancora chiuse, leaverage ci dice quanto siamo disposti ad investire in base agli introiti,% t non ho capito
			il guadagno settimanale è la transazione monetaria che c'è stata.
			Come si può vedere dall' esempio la nostra scelte di trading secondo il nostro algoritmo è molto ottimistica rispetto a ciò che è successo realmente, anche se a grandi linee comunque rispetta ciò che è successo, dandoci però un guadagno falsato.
			La modifica dell' algoritmo, oltre ad essere permessa da qualsiasi pc essendo tutto in cloud, viene eseguita da una server farm che sicuramente è molto più performante del nostro pc infatti quello che si vede in figura solo le valutazioni del mercato americano in circa 12 anni, un onere di computazione non banale.
			Una possibile modifica dell' algoritmo è  puntare solo sulle azioni che ci danno un valore di utile, maggiore:
			\includegraphics[width=1.0\textwidth, height=0.40\textheight]{factorfilter0,1.png} 
			Come vediamo l' algoritmo si è comportato meglio, perché ho preso solo le 3000 azioni che mi davano un' utilità sperata maggiore, quindi ho fatto trading su azioni che mi permettono di massimizzare i guadagni, ovviamente è un operazione che va bene a lungo termine, perché altrimenti non è detto che quell' azione abbia il comportamento sperato, può darsi che la sua efficienza sia minore, dato che fattori non ancora accaduti potrebbe in realtà portare ad un' utilità diversa.
			Un altro esperimento è stato quello di aumentare il profitto che si voleva fare con una determinata azione, tendendo quindi a cercare di sfruttare quando più quell' azione per massimizzare i guadagni,il risultato però è stato peggiore:
			\includegraphics[width=1.0\textwidth, height=0.40\textheight]{stock2.png} 
			Possiamo vedere che il numero di transazioni sono state minori rispetto ai due casi precedenti, appunto perché una volta che si è iniziato a fare un trade su quell' azione fino a che non ci porta al guadagno voluto o scende al di sotto di un valore minimo non la vendiamo, così abbiamo cercato di attuare una politica più conservativa sulla compravendita, che ci ha portato a dei ricavi minori, ma ad un atteggiamento più consono a quello che in realtà è successo.